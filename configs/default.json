{
  "Mic": {
    "params": {
      "audio_device": "default",
      "samplerate": 16000,
      "buffer_size": 640,
      "channels": 1
    }
  },
  "Vad": {
    "params": {
      "samplerate": 16000,
      "repo_or_dir": "snakers4/silero-vad",
      "model_name": "silero_vad",
      "force_reload": false,
      "use_onnx": true,
      "no_voice_wait_sec": 1,
      "onnx_verbose": false,
      "verbose": false
    }
  },
  "Stt_Whisper": {
    "params": {
      "device": "cuda:0",
      "model_name": "openai/whisper-large-v3-turbo",
      "low_cpu_mem_usage": true,
      "attn": "flash_attention_2",
      "verbose": false,
      "language": "fr"
    }
  },
  "Llm_Llama_local": {
    "params": {
      "backend_type": "local",
      "device": "gpu",
      "custom_path": "",
      "model_name": "mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF",
      "model_file": "meta-llama-3.1-8b-instruct-abliterated.Q8_0.gguf",
      "num_gpu_layers": -1,
      "context_length": 8192,
      "streaming_output": true,
      "chat_format": "llama-3",
      "system_message": "Tu es Aria, une IA sarcastique. RÈGLE CRITIQUE : Si l'utilisateur pose une question sur la météo ou la température d'une ville, tu DOIS utiliser l'outil 'get_weather' avant de répondre. Ne devine jamais la météo par toi-même.",
      "verbose": false
    }
  },
  "Llm_Ministral_local": {
    "params": {
      "backend_type": "local",
      "device": "cuda:0",
      "custom_path": "",
      "model_name": "Ministral-8B-Instruct-2410-Q4_K_S",
      "model_file": "huihui-ministral-3-3b-reasoning-2512-abliterated-q4_k_m.gguf",
      "num_gpu_layers": -1,
      "context_length": 32768,
      "streaming_output": true,
      "chat_format": "mistral-instruct",
      "system_message": "Tu es Aria, une IA sarcastique. RÈGLE CRITIQUE : Si l'utilisateur pose une question sur la météo ou la température d'une ville, tu DOIS utiliser l'outil 'get_weather' avant de répondre. Ne devine jamais la météo par toi-même.",
      "verbose": false
    }
  },
  "Llm_Ministral_lmstudio": {
    "params": {
      "backend_type": "llm_studio",
      "base_url": "http://192.168.1.44:1234/v1",
      "device": "cuda:0",
      "custom_path": "",
      "model_name": "mistralai/ministral-3-14b-reasoning",
      "model_file": "",
      "num_gpu_layers": -1,
      "context_length": 32768,
      "streaming_output": true,
      "chat_format": "mistral-instruct",
      "system_message": "Tu es Aria, une IA sarcastique. RÈGLE CRITIQUE : Si l'utilisateur pose une question sur la météo ou la température d'une ville, tu DOIS utiliser l'outil 'get_weather' avant de répondre. Ne devine jamais la météo par toi-même.",
      "verbose": false
    }
  },
  "Llm_Ministral_ollama": {
    "params": {
      "backend_type": "ollama",
      "base_url": "http://192.168.1.44:11434/v1",
      "device": "cuda:0",
      "custom_path": "",
      "model_name": "mistral:7b",
      "model_file": "",
      "num_gpu_layers": -1,
      "context_length": 32768,
      "streaming_output": true,
      "chat_format": "mistral-instruct",
      "system_message": "Tu es Aria. Réponds directement à l'utilisateur de manière concise. Ne montre jamais les appels de fonctions techniques ou de JSON dans ta réponse finale. Tu dois utiliser des outils si ils sont pertinant dans la réponse. Réponds uniquement avec du texte naturel.",
      "verbose": false
    }
  },
  "Mcp": {
    "params": {
      "servers": {
        "meteo_local": {
          "run": "uv",
          "args": [
            "run",
            "/aria/tools/weather_server.py"
          ],
          "env": {
            "API_KEY": "12345"
          }
        }
      }
    }
  },
  "Tts_Kokoro": {
    "params": {
      "device": "cpu",
      "tts_type": "kokoro",
      "use_deepspeed": false,
      "text_splitting": false,
      "model_name": "tts_models/multilingual/multi-dataset/xtts_v2",
      "force_reload": false,
      "verbose": false,
      "kokoro_voice": "ff_siwis",
      "kokoro_voice_speed": 1.2,
      "kokoro_lang_code": "f",
      "static": {
        "voice_to_clone": "static/sofia_hellen.wav"
      }
    }
  },
  "Tts_Sherpa": {
    "params": {
      "device": "cpu",
      "tts_type": "sherpa",
      "model_dir": "/aria/models/vits-piper-fr_FR-upmc-medium",
      "speed": 0.8,
      "noise_scale": 0.667,
      "noise_scale_w": 0.8,
      "static": {
        "voice_to_clone": "static/Hellena_science.wav"
      }
    }
  },
  "Ap": {
    "params": {
      "audio_device": "default",
      "samplerate": 24000,
      "buffer_size": 960,
      "channels": 1,
      "static": {
        "listening_sound": "static/listening.wav",
        "transition_sound": "static/transition.wav"
      }
    }
  }
}