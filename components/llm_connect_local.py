# components/llm_connect_local.py
import os
import sys
import time
import json
from llama_cpp import Llama
from huggingface_hub import hf_hub_download
import components.utils as utils
from .utils import clean_text_for_tts, remove_nonverbal_cues
from .mcp_manager import McpManager

class LlmConnectLocal:
    def __init__(self, params, all_config=None):
        self.params = params or {}

        self.device = self.params.get("device", "gpu")
        self.num_gpu_layers = self.params.get("num_gpu_layers", -1)
        self.chat_format = self.params.get("chat_format", "mistral-instruct")
        self.system_message = self.params.get("system_message", "")
        self.verbose = self.params.get("verbose", False)

        # Configuration des chemins
        self.model_dir = "/aria/models/llm"
        self.model_name = self.params.get("model_name", None)
        self.model_file = self.params.get("model_file", None)
        self.model_path = os.path.join(self.model_dir, self.model_file)
        self._ensure_model_download()

        # Initialisation MCP sÃ©curisÃ©e (Ã©vite le blocage si un serveur est HS)
        self.mcp = None
        if all_config and "Mcp" in all_config:
            try:
                self.mcp = McpManager(all_config["Mcp"]["params"])
                self.mcp.start()
            except Exception as e:
                utils.log_perf("LLM-LOCAL", f"Alerte: MCP n'a pas pu dÃ©marrer: {e}")

        # Initialisation de Llama avec gestion explicite du GPU
        main_gpu = 0 if "cuda" in self.device or self.device == "gpu" else None
        
        utils.log_perf("LLM-LOCAL", f"Chargement modele {self.model_name} sur {self.device}...")
        self.llm = Llama(
            model_path=self.model_path,
            n_gpu_layers=self.num_gpu_layers,
            n_ctx=4096,
            chat_format=self.chat_format,
            verbose=self.verbose,
            main_gpu=main_gpu
        )

        self.user_aware_messages = {}

    def _ensure_model_download(self):
        """VÃ©rifie ou tÃ©lÃ©charge le modÃ¨le GGUF."""
        if not os.path.exists(self.model_path):
            utils.log_perf("LLM-LOCAL", f"ðŸ“¥ TÃ©lÃ©chargement HF : {self.model_name}")
            os.makedirs(self.model_dir, exist_ok=True)
            hf_hub_download(
                repo_id=self.model_name,
                filename=self.model_file,
                local_dir=self.model_dir
            )
            utils.log_perf("LLM-LOCAL", "âœ… ModÃ¨le rÃ©cupÃ©rÃ©.")

    def get_answer_web(self, tts, query, user):
        """GÃ©nÃ¨re une rÃ©ponse en gÃ©rant les outils MCP et le streaming TTS."""

        start_time = time.time()

        if user not in self.user_aware_messages:
            self.user_aware_messages[user] = [{"role": "system", "content": self.system_message}]

        self.user_aware_messages[user].append({"role": "user", "content": query})
        
        # 1. Ã‰TAPE DE DÃ‰CISION (TOOLS)
        # Correction CRITIQUE : S'assurer que 'tools' est une liste ou None, JAMAIS un boolÃ©en.
        tools = None
        if self.mcp:
            try:
                mcp_tools = self.mcp.get_tools_schema()
                # On ne garde que si c'est une liste non vide
                if isinstance(mcp_tools, list) and len(mcp_tools) > 0:
                    tools = mcp_tools
            except Exception as e:
                utils.log_perf("LLM-LOCAL", f"Erreur lecture schema outils: {e}")

        # Premier passage pour dÃ©tecter si un outil doit Ãªtre appelÃ©
        try:
            # On construit les paramÃ¨tres dynamiquement pour Ã©viter d'envoyer tools=None ou tools=[]
            completion_params = {
                "messages": self.user_aware_messages[user],
                "temperature": 0.1
            }
            if tools:
                completion_params["tools"] = tools
                completion_params["tool_choice"] = "auto"

            response = self.llm.create_chat_completion(**completion_params)
        except Exception as e:
            utils.log_perf("LLM-LOCAL", f"Erreur completion (Ã©tape tools): {e}")
            yield "DÃ©solÃ©e, j'ai rencontrÃ© une erreur technique.", None
            return

        message = response["choices"][0]["message"]

        # 2. EXÃ‰CUTION DES OUTILS SI NÃ‰CESSAIRE
        if "tool_calls" in message and message["tool_calls"] and self.mcp:
            self.user_aware_messages[user].append(message)
            
            utils.log_perf("LLM-LOCAL", f"   Debut Appel des outils MCP...")
            for tool_call in message["tool_calls"]:
                fn_name = tool_call["function"]["name"]
                try:
                    fn_args = json.loads(tool_call["function"]["arguments"])
                except Exception:
                    fn_args = {}
                
                tool_result = self.mcp.call_tool_sync(fn_name, fn_args)
                
                self.user_aware_messages[user].append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": str(tool_result)
                })
            utils.log_perf("LLM-LOCAL", f"   Fin Appel des outils MCP.")

        # 3. GÃ‰NÃ‰RATION DE LA RÃ‰PONSE FINALE EN STREAMING
        try:
            outputs = self.llm.create_chat_completion(
                messages=self.user_aware_messages[user],
                stream=True,
                temperature=0.7
            )
        except Exception as e:
            utils.log_perf("LLM-LOCAL", f"Erreur completion (Ã©tape finale): {e}")
            yield "Mon cerveau vient de griller.", None
            return

        full_response = ""
        tts_text_buffer = ""
        
        token_count = 0
        for chunk in outputs:
            delta = chunk["choices"][0].get("delta", {})
            if "content" in delta:
                token = delta["content"]
                full_response += token
                tts_text_buffer += token
                token_count += 1 # On compte chaque morceau gÃ©nÃ©rÃ©

                if any(punc in token for punc in [".", "!", "?", "\n"]):
                    txt_to_speak = clean_text_for_tts(remove_nonverbal_cues(tts_text_buffer)).strip()
                    if len(txt_to_speak) > 3:
                        audio_path = tts.run_tts_to_file(txt_to_speak, user_id=user)
                        tts_text_buffer = ""
                        yield full_response, audio_path
                    else:
                        yield full_response, None
                else:
                    yield full_response, None

        final_txt = clean_text_for_tts(remove_nonverbal_cues(tts_text_buffer)).strip()
        if len(final_txt) > 1:
            audio_path = tts.run_tts_to_file(final_txt, user_id=user)
            yield full_response, audio_path

        # Calcul des performances Ã  la fin de la gÃ©nÃ©ration
        end_time = time.time()
        duration = end_time - start_time
        tokens_per_sec = token_count / duration if duration > 0 else 0
        
        # Log final dans la console
        utils.log_perf("LLM-LOCAL", f"   Vitesse : {tokens_per_sec:.2f} t/s ({token_count} tokens en {duration:.2f}s)")


        self.user_aware_messages[user].append({"role": "assistant", "content": full_response})